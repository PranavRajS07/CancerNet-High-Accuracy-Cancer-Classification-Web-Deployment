{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7lysYugaUMEi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, AveragePooling2D, Flatten, GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3i_KzTZUMsu",
        "outputId": "1f89a468-bbe6-46d6-af6d-f3e42578afb1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Get the current working directory\n",
        "current_directory = os.getcwd()\n",
        "print(\"Current working directory:\", current_directory)\n",
        "# List the contents of the current directory\n",
        "contents = os.listdir(current_directory)\n",
        "print(\"Contents of the current directory:\", contents)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiFfDZZMUMv_",
        "outputId": "2a1b8798-5185-43bd-ca2a-8e2b13bc7261"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content\n",
            "Contents of the current directory: ['.config', 'drive', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Specify the full path to your main dataset zip file\n",
        "main_zip_file_path = '/content/drive/MyDrive/lc.zip'\n",
        "\n",
        "# Specify the extraction directory\n",
        "extraction_directory = '/content'  # Change this to your desired extraction directory\n",
        "\n",
        "try:\n",
        "    # Unzip the main dataset\n",
        "    with zipfile.ZipFile(main_zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extraction_directory)\n",
        "    print(f\"Dataset '{main_zip_file_path}' has been successfully extracted to '{extraction_directory}'.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File '{main_zip_file_path}' not found. Please check the file path.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LnifjdcUMyk",
        "outputId": "02248115-0de9-467a-e87f-bdd2262bba3c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset '/content/drive/MyDrive/lc.zip' has been successfully extracted to '/content'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Define the paths to your train and test directories\n",
        "train_dir = '/content/Lung and Colon Cancer/train'  # Replace with your actual train directory path\n",
        "test_dir = '/content/Lung and Colon Cancer/test'\n",
        "# Replace with your actual test directory path\n",
        "# Get the class names (subdirectories) in the train directory\n",
        "train_class_names = [class_name for class_name in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, class_name))]\n",
        "# Get the class names (subdirectories) in the test directory\n",
        "test_class_names = [class_name for class_name in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, class_name))]\n",
        "# Print the class names in the train and test directories\n",
        "print(\"Class names in the train directory:\")\n",
        "print(train_class_names)\n",
        "print(\"\\nClass names in the test directory:\")\n",
        "print(test_class_names)"
      ],
      "metadata": {
        "id": "qyU34zndUMzY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d78452d7-2cd4-484b-b9de-be6e0db9baba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class names in the train directory:\n",
            "['lung_bnt', 'colon_aca', 'colon_bnt', 'lung_aca']\n",
            "\n",
            "Class names in the test directory:\n",
            "['lung_bnt', 'colon_aca', 'colon_bnt', 'lung_aca']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zLzdDF3VVGaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = (224, 224)  # Adjust as needed\n",
        "num_classes = 4 # Change to the number of classes in your dataset\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "momentum = 0.9\n"
      ],
      "metadata": {
        "id": "Wm6IT3VuUM1B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "# Load training and testing data using data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2  # Use a portion of the data for validation\n",
        ")\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n"
      ],
      "metadata": {
        "id": "tW6ADlJ1UM1n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "304cdb72-190e-493b-a0c4-9b2d91dba628"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9600 images belonging to 4 classes.\n",
            "Found 2400 images belonging to 4 classes.\n",
            "Found 8000 images belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Conv2D, MaxPooling2D, AveragePooling2D, Dropout, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "def Inception_block(X, f1, f2_conv1, f2_conv3, f3_conv1, f3_conv5, f4):\n",
        "    # Branch 1\n",
        "    branch1 = Conv2D(f1, (1, 1), padding='same', activation='relu')(X)\n",
        "    # Branch 2\n",
        "    branch2 = Conv2D(f2_conv1, (1, 1), padding='same', activation='relu')(X)\n",
        "    branch2 = Conv2D(f2_conv3, (3, 3), padding='same', activation='relu')(branch2)\n",
        "    # Branch 3\n",
        "    branch3 = Conv2D(f3_conv1, (1, 1), padding='same', activation='relu')(X)\n",
        "    branch3 = Conv2D(f3_conv5, (5, 5), padding='same', activation='relu')(branch3)\n",
        "    # Branch 4\n",
        "    branch4 = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(X)\n",
        "    branch4 = Conv2D(f4, (1, 1), padding='same', activation='relu')(branch4)\n",
        "    # Concatenate the branches\n",
        "    return Concatenate(axis=-1)([branch1, branch2, branch3, branch4])\n",
        "\n",
        "def GoogLeNet(input_shape=(224, 224, 3), num_classes=4):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    # convolutional layer: filters = 64, kernel_size = (7,7), strides = 2\n",
        "    X = Conv2D(filters=64, kernel_size=(7, 7), strides=2, padding='valid', activation='relu')(input_layer)\n",
        "    # max-pooling layer: pool_size = (3,3), strides = 2\n",
        "    X = MaxPooling2D(pool_size=(3, 3), strides=2)(X)\n",
        "    # convolutional layer: filters = 64, strides = 1\n",
        "    X = Conv2D(filters=64, kernel_size=(1, 1), strides=1, padding='same', activation='relu')(X)\n",
        "    # convolutional layer: filters = 192, kernel_size = (3,3)\n",
        "    X = Conv2D(filters=192, kernel_size=(3, 3), padding='same', activation='relu')(X)\n",
        "    # max-pooling layer: pool_size = (3,3), strides = 2\n",
        "    X = MaxPooling2D(pool_size=(3, 3), strides=2)(X)\n",
        "    # 1st Inception block\n",
        "    X = Inception_block(X, f1=64, f2_conv1=96, f2_conv3=128, f3_conv1=16, f3_conv5=32, f4=32)\n",
        "    # 2nd Inception block\n",
        "    X = Inception_block(X, f1=128, f2_conv1=128, f2_conv3=192, f3_conv1=32, f3_conv5=96, f4=64)\n",
        "    # max-pooling layer: pool_size = (3,3), strides = 2\n",
        "    X = MaxPooling2D(pool_size=(3, 3), strides=2)(X)\n",
        "    # 3rd Inception block\n",
        "    X = Inception_block(X, f1=192, f2_conv1=96, f2_conv3=208, f3_conv1=16, f3_conv5=48, f4=64)\n",
        "    # Extra network 1:\n",
        "    X1 = AveragePooling2D(pool_size=(5, 5), strides=3)(X)\n",
        "    X1 = Conv2D(filters=128, kernel_size=(1, 1), padding='same', activation='relu')(X1)\n",
        "    X1 = Flatten()(X1)\n",
        "    X1 = Dense(1024, activation='relu')(X1)\n",
        "    X1 = Dropout(0.7)(X1)\n",
        "    X1 = Dense(4, activation='softmax')(X1)  # Assuming you have 4 classes\n",
        "    # 4th Inception block\n",
        "    X = Inception_block(X, f1=160, f2_conv1=112, f2_conv3=224, f3_conv1=24, f3_conv5=64, f4=64)\n",
        "    # 5th Inception block\n",
        "    X = Inception_block(X, f1=128, f2_conv1=128, f2_conv3=256, f3_conv1=24, f3_conv5=64, f4=64)\n",
        "    # 6th Inception block\n",
        "    X = Inception_block(X, f1=112, f2_conv1=144, f2_conv3=288, f3_conv1=32, f3_conv5=64, f4=64)\n",
        "    # Extra network 2:\n",
        "    X2 = AveragePooling2D(pool_size=(5, 5), strides=3)(X)\n",
        "    X2 = Conv2D(filters=128, kernel_size=(1, 1), padding='same', activation='relu')(X2)\n",
        "    X2 = Flatten()(X2)\n",
        "    X2 = Dense(1024, activation='relu')(X2)\n",
        "    X2 = Dropout(0.7)(X2)\n",
        "    X2 = Dense(1000, activation='softmax')(X2)\n",
        "    # 7th Inception block\n",
        "    X = Inception_block(X, f1=256, f2_conv1=160, f2_conv3=320, f3_conv1=32, f3_conv5=128, f4=128)\n",
        "    # max-pooling layer: pool_size = (3,3), strides = 2\n",
        "    X = MaxPooling2D(pool_size=(3, 3), strides=2)(X)\n",
        "    # 8th Inception block\n",
        "    X = Inception_block(X, f1=256, f2_conv1=160, f2_conv3=320, f3_conv1=32, f3_conv5=128, f4=128)\n",
        "    # 9th Inception block\n",
        "    X = Inception_block(X, f1=384, f2_conv1=192, f2_conv3=384, f3_conv1=48, f3_conv5=128, f4=128)\n",
        "    # Global Average pooling layer\n",
        "    X = GlobalAveragePooling2D(name='GAPL')(X)\n",
        "    # Fully connected output layer\n",
        "    output_layer = Dense(num_classes, activation='softmax')(X)\n",
        "    return Model(inputs=input_layer, outputs=output_layer)\n",
        "# Create the GoogLeNet model\n",
        "model = GoogLeNet(input_shape=(224, 224, 3), num_classes=4)  # Assuming you have 4 classes\n",
        "\n"
      ],
      "metadata": {
        "id": "4-uMLJPmUM2f"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the GoogLeNet model\n",
        "model = GoogLeNet(input_shape=(224, 224, 3), num_classes=num_classes)\n",
        "\n",
        "# Compile the model using the legacy SGD optimizer and 'categorical_crossentropy' loss\n",
        "sgd = SGD(learning_rate=learning_rate, momentum=momentum, nesterov=True)\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "model.fit(train_generator, epochs=20, validation_data=validation_generator, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "id": "rHrxHYsAUM9P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "510d93e5-0a78-419c-bab9-1e642519355d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "300/300 [==============================] - 200s 589ms/step - loss: 1.3833 - accuracy: 0.3113 - val_loss: 1.3751 - val_accuracy: 0.4700\n",
            "Epoch 2/20\n",
            "300/300 [==============================] - 180s 601ms/step - loss: 1.3086 - accuracy: 0.5152 - val_loss: 1.1592 - val_accuracy: 0.4142\n",
            "Epoch 3/20\n",
            "300/300 [==============================] - 178s 593ms/step - loss: 0.8404 - accuracy: 0.6472 - val_loss: 0.6304 - val_accuracy: 0.7538\n",
            "Epoch 4/20\n",
            "300/300 [==============================] - 175s 583ms/step - loss: 0.4802 - accuracy: 0.8076 - val_loss: 0.3881 - val_accuracy: 0.8500\n",
            "Epoch 5/20\n",
            "300/300 [==============================] - 179s 595ms/step - loss: 0.3506 - accuracy: 0.8558 - val_loss: 0.3947 - val_accuracy: 0.8254\n",
            "Epoch 6/20\n",
            "300/300 [==============================] - 179s 596ms/step - loss: 0.3069 - accuracy: 0.8732 - val_loss: 0.3090 - val_accuracy: 0.8733\n",
            "Epoch 7/20\n",
            "300/300 [==============================] - 176s 586ms/step - loss: 0.2752 - accuracy: 0.8869 - val_loss: 0.2656 - val_accuracy: 0.8900\n",
            "Epoch 8/20\n",
            "300/300 [==============================] - 177s 591ms/step - loss: 0.2146 - accuracy: 0.9155 - val_loss: 0.1834 - val_accuracy: 0.9304\n",
            "Epoch 9/20\n",
            "300/300 [==============================] - 183s 611ms/step - loss: 0.1767 - accuracy: 0.9305 - val_loss: 0.1696 - val_accuracy: 0.9392\n",
            "Epoch 10/20\n",
            "300/300 [==============================] - 177s 590ms/step - loss: 0.1493 - accuracy: 0.9463 - val_loss: 0.1377 - val_accuracy: 0.9492\n",
            "Epoch 11/20\n",
            "300/300 [==============================] - 175s 583ms/step - loss: 0.1329 - accuracy: 0.9497 - val_loss: 0.1235 - val_accuracy: 0.9529\n",
            "Epoch 12/20\n",
            "300/300 [==============================] - 177s 591ms/step - loss: 0.1138 - accuracy: 0.9615 - val_loss: 0.1583 - val_accuracy: 0.9358\n",
            "Epoch 13/20\n",
            "300/300 [==============================] - 172s 574ms/step - loss: 0.1008 - accuracy: 0.9650 - val_loss: 0.1129 - val_accuracy: 0.9517\n",
            "Epoch 14/20\n",
            "300/300 [==============================] - 176s 588ms/step - loss: 0.0936 - accuracy: 0.9689 - val_loss: 0.0855 - val_accuracy: 0.9712\n",
            "Epoch 15/20\n",
            "300/300 [==============================] - 169s 563ms/step - loss: 0.0828 - accuracy: 0.9722 - val_loss: 0.0899 - val_accuracy: 0.9688\n",
            "Epoch 16/20\n",
            "300/300 [==============================] - 170s 567ms/step - loss: 0.0841 - accuracy: 0.9702 - val_loss: 0.0690 - val_accuracy: 0.9771\n",
            "Epoch 17/20\n",
            "300/300 [==============================] - 168s 561ms/step - loss: 0.0746 - accuracy: 0.9735 - val_loss: 0.0678 - val_accuracy: 0.9771\n",
            "Epoch 18/20\n",
            "300/300 [==============================] - 170s 566ms/step - loss: 0.0667 - accuracy: 0.9762 - val_loss: 0.1178 - val_accuracy: 0.9642\n",
            "Epoch 19/20\n",
            "300/300 [==============================] - 169s 564ms/step - loss: 0.0584 - accuracy: 0.9810 - val_loss: 0.0622 - val_accuracy: 0.9737\n",
            "Epoch 20/20\n",
            "300/300 [==============================] - 170s 567ms/step - loss: 0.0618 - accuracy: 0.9781 - val_loss: 0.0428 - val_accuracy: 0.9875\n",
            "250/250 [==============================] - 34s 136ms/step - loss: 0.0405 - accuracy: 0.9889\n",
            "Test Loss: 0.04050389677286148, Test Accuracy: 0.9888749718666077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace 'lcgnet.h5' with your desired model file name\n",
        "model.save('lungcolongnet.h5', save_format='h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPu-ULvtlK0-",
        "outputId": "cf4cde42-3497-440e-f3a7-d829f3a11f34"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R8m0dm9qlK2H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}